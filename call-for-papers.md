---
title: Call for Papers
layout: default
---

# Call for Papers

We develop large models to “understand” images, videos and natural language that fuel many intelligent applications from text completion to self-driving cars. But tabular data has long been overlooked despite its dominant presence in data-intensive systems. By learning latent representations from (semi-)structured tabular data, pretrained table models have shown preliminary but impressive performance for semantic parsing, question answering, table understanding, and data preparation [1,2,3,4,5]. As these early advances reveal a huge potential for making an impact on various downstream applications, the time has come to consider tabular data as a first-class modality for representation learning and stimulate advances in this direction. The Table Representation Learning workshop is the first workshop in this emerging research area and is centred around three main goals:

1) **Motivate tabular data as a first-class modality** for representation learning and further shaping this area.

2) **Showcase impactful applications** of pretrained table models and discussing future opportunities thereof.

3) **Foster discussion and collaboration** across the machine learning, natural language processing, and data management communities.


## Scope

We invite submissions that address, but are not limited to, any of the following topics on machine learning for tabular data:
- **Representation Learning** Representation learning techniques for structured (e.g., relational databases) or semi-structured (Web tables, spreadsheet tables) tabular data. This includes developing specialized data encodings or adaptation of general-purpose ones (e.g., GPT-3) for tabular data, multimodal learning across tables, and other modalities (e.g., natural language, images, code), and relevant fine-tuning and prompting strategies.
- **Downstream Applications** Machine learning applications involving tabular data, such as data preparation (e.g. data cleaning, integration, cataloging, anomaly detection), retrieval (e.g., semantic parsing, question answering, fact-checking), information extraction, and generation (e.g., table-to-text).
- **Upstream Applications** Applications that use representation learning to optimize tabular data processing systems, such as table parsers (extracting tables from documents, spreadsheets, presentations, images), storage (e.g. compression, indexing), and querying (e.g. query plan optimization, cost estimation).
- **Industry Papers** Applications of tabular representation models in production. Challenges of maintaining and managing table representation models in a fast evolving context, e.g. data updating, error correction, monitoring.
- **New Resources** Survey papers, benchmarks and datasets for tabular representation models and their applications.
- **Others** Formalization, surveys, visions and reflections to structure and guide future research.


## Important dates
- Submission open: 20 August 2022
- Submission deadline: <b>20 September 2022</b>
- Notifications: 20 October 2022
- Camera-ready, slides and recording upload: 3 November 2022
- Workshop: 2 December 2022


## Submission formats
The workshop will accept regular research papers and industrial papers. Submissions should follow the NeurIPS proceedings format and choose the suitable category of:
- Abstract: 1 page + references (open challenges, reflections, and thought-provoking visions).
- Extended abstract: at most 4 pages + references.
- Regular paper: at least 6 pages + references.


## Novelty and conflicts

The workshop does not accept submissions that have previously been published at NeurIPS or other machine learning or related venues. We do invite submissions that have been published in, for example, data management venues. Authors of submitted work will be asked to mark (domain) conflicts of interest with the workshop organizers and the program committee, and reviewing will be handled accordingly.


## Submission and review process

The submission should be uploaded through the <a href="https://openreview.net/group?id=NeurIPS.cc/2022/Workshop/TRL" target="blank">TRL Workshop page on OpenReview</a>. Papers will be reviewed in a single-anonymous manner. Reviewers will recommend submissions for oral or poster presentations. Accepted papers will be published on the website but the workshop is non-archival.


## References

[1] Yin, P., Neubig, G., Yih, W. T., & Riedel, S. TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020*.

[2] Herzig, J., Nowak, P. K., Mueller, T., Piccinno, F., & Eisenschlos, J. TaPas: Weakly Supervised Table Parsing via Pre-training. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020*.

[3] Deng, X., Sun, H., Lees, A., Wu, Y., & Yu, C. TURL: Table Understanding through Representation Learning. *Proceedings of the VLDB Endowment, 2021*.

[4] Tang, N., Fan, J., Li, F., Tu, J., Du, X., Li, G., ... & Ouzzani, M. RPT: relational pre-trained transformer is almost all you need towards democratizing data preparation. *Proceedings of the VLDB Endowment*, 2021.

[5] Ilyas, I. F., & Rekatsinas T. Machine Learning and Data Cleaning: Which Serves the Other? *Journal of Data and Information Quality*, 2021.
