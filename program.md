---
title: Program
layout: default
---

# Program

The workshop will be hosted physically in New Orleans. We are are pleased to host all speakers in-person, and authors of accepted papers will present their work in-person as well in a talk or during the poster sessions. While we welcome all other attendees to join us in New Orleans physically, it is also possible to attend the program virtually as the sessions will be live-streamed!

## Speakers
<!-- We are pleased to host the following speakers:
<a href="https://ai.facebook.com/people/alon-halevy/" target="blank">Alon Halevy (keynote)</a>, Meta AI
    <details><summary>Bio</summary>
        Alon Halevy has been a Director at Facebook AI since August 2019. He works on Affective Computing and on data management for artificial intelligence, including the combination of neural and symbolic techniques for data management. Prior to Facebook, he was the CEO of Megagon Labs (2015-2018) and led the Structured Data Research Group at Google Research (2005-2015), where they developed WebTables and Google Fusion Tables. From 1998-2005 he was a professor at the University of Washington, where he founded the database group. Before that, he was at AT&T Bell Labs (and AT&T Labs) (1993-1997). He founded two startups, Nimble Technology and Transformic Inc. (acquired by Google in 2005). He received his Ph.D in Computer Science from Stanford in 1993 and his Bachelors in Computer Science and Mathematics from the Hebrew University of Jerusalem in 1988. He has authored two books: The Infinite Emotions of Coffee (December, 2011) and Principles of Data Integration (with AnHai Doan and Zack Ives, published in 2012). He is a Fellow of the ACM and a recipient of the PECASE Award and Sloan Fellowship. He and his co-authors received VLDB 10-year Best Paper Awards for their 2008 paper on WebTables and for their 1996 paper on the Information Manifold Data Integration System.
    </details>
<a href="http://www.phontron.com/" target="blank">Graham Neubig (keynote)</a>, Carnegie Mellon University
    <details><summary>Bio</summary>
    Graham Neubig is an associate professor at the Language Technologies Institute of Carnegie Mellon University. His research focuses on multilingual natural language processing, natural language interfaces to computers, and machine learning methods for NLP, with the final goal of every person in the world being able to communicate with each-other, and with computers in their own language. He also contributes to making NLP research more accessible through open publishing of research papers, advanced NLP course materials and video lectures, and open-source software, all of which are available on his web site.
    </details>
<a href="https://www.informatik.tu-darmstadt.de/datamanagement/datamanagement/index.en.jsp" target="blank">Carsten Binnig</a>, TU Darmstadt
    <details><summary>Bio</summary>
    Carsten Binnig is a Full Professor in the Computer Science department at at TU Darmstadt and an Adjunct Associate Professor in the Computer Science department at Brown University. Carsten received his PhD at the University of Heidelberg in 2008. Afterwards, he spent time as a postdoctoral researcher in the Systems Group at ETH Zurich and at SAP working on in-memory databases. Currently, his research focus is on the design of scalable data management systems, databases and modern hardware as well as machine learning for scalable systems. His work has been awarded with a Google Faculty Award, as well as multiple best paper and best demo awards for his research.
    </details>
<a href="https://www.microsoft.com/en-us/research/people/beichen/" target="blank">Bei Chen</a>, Microsoft Research
    <details><summary>Bio</summary>
    Bei Chen (陈蓓) is a senior researcher at Microsoft Research Asia. She joined Microsoft in 2017 after receiving her Ph.D. degree from Department of Computer Science and Technology in Tsinghua University. Her research interests are primarily on machine learning and its applications in natural language processing and data mining, especially latent feature models, probabilistic graphical models, Bayesian nonparametrics, reinforcement learning and deep learning.
    </details>
<a href="https://hci.stanford.edu/~cagatay/" target="blank">Çağatay Demiralp</a>, Sigma Computing
    <details><summary>Bio</summary>
    Çağatay is Chief Research Scientist at Sigma Computing. Previously, he was a senior research scientist at Megagon Labs, a visiting researcher with the data systems group at MIT CSAIL, and a research staff member at IBM Research. Between 2012-2014, he was a postdoctoral scholar at Stanford and member of IDL at the University of Washington. Çağatay obtained his PhD from Brown University and also co-founded Fitnescity, a startup providing easy access and data analytics for wellness lab tests.
    His current research focuses on solving problems at the intersection of Data Systems + Artificial Intelligence + Human-Computer Interaction at scale.
    </details>
<a href="http://web.cse.ohio-state.edu/~sun.397/" target="blank">Huan Sun</a>, Ohio State University
    <details><summary>Bio</summary>
    Huan Sun is an assistant professor in the Department of Computer Science
    and Engineering at the Ohio State University. She was a visiting scientist at
    the University of Washington in the first half of 2016, and received a Ph.D.
    in Computer Science from University of California, Santa Barbara (2015)
    and a B.S. in EEIS from the University of Science and Technology of China
    (2010). Her research interests lie in data mining and machine learning, with
    emphasis on question answering, text mining and understanding, network
    analysis, and human behavior understanding. Huan received the SIGKDD
    Ph.D. Dissertation Runner-Up Award (2016), the honor of being MIT EECS
    Rising Stars (2015), the UC Regents’ Special Fellowship (2010, 2014), and
    the CS Ph.D. Progress Award (2014).
    </details>
<a href="https://jungyhuk.github.io/" target="blank">Xinyun Chen</a>, Google Brain
    <details><summary>Bio</summary>
    Xinyun Chen is a senior research scientist at Google Brain. She obtained her Ph.D. degree at UC Berkeley, working with Prof. Dawn Song. Her research lies at the intersection of deep learning, programming languages, and security. Her recent research focuses on neural program synthesis and adversarial machine learning. She received the Facebook Fellowship in 2020, and Rising Stars in Machine Learning in 2021. Her work SpreadsheetCoder for spreadsheet formula prediction was integrated into Google Sheets, and she was part of the AlphaCode team when she interned at DeepMind.
    </details> -->


<table border="0" style="border:none; border-collapse:collapse; width: 100%; cellspacing:0; cellpadding:0" >
    <tr style="border:none"  align="left">
      <td style="border:none" width="16%"><a href="https://ai.facebook.com/people/alon-halevy/" target="blank"><img src="assets/ah.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>
      <td style="border:none" width="16%"><a href="http://www.phontron.com/" target="blank"><img src="assets/gn.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>
      <td style="border:none" width="16%"><a href="https://jungyhuk.github.io/" target="blank"><img src="assets/xc.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>   
      <td style="border:none" width="16%"><a href="https://www.informatik.tu-darmstadt.de/fb20/organisation_fb20/professuren_und_gruppenleitungen/fb20professuren_und_gruppenleitungen_detailseite_21760.de.jsp" target="blank"><img src="assets/cb.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>   
      <td style="border:none" width="16%"><a href="http://web.cse.ohio-state.edu/~sun.397/" target="blank"><img src="assets/hs.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>
      <!-- <td style="border:none" width="16%"><a href="https://hci.stanford.edu/~cagatay/" target="blank"><img src="assets/cd.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td> -->
    </tr>
    <tr style="border:none" align="left">
      <td style="border:none" bgcolor="white"><a href="https://ai.facebook.com/people/alon-halevy/" target="blank">Alon Halevy<br>Meta AI</a></td>
      <td style="border:none" bgcolor="white"><a href="http://www.phontron.com/" target="blank">Graham Neubig<br>CMU</a></td>
      <td style="border:none" bgcolor="white"><a href="https://jungyhuk.github.io/" target="blank">Xinyun Chen<br>Google Brain</a></td>
      <td style="border:none" bgcolor="white"><a href="https://www.informatik.tu-darmstadt.de/fb20/organisation_fb20/professuren_und_gruppenleitungen/fb20professuren_und_gruppenleitungen_detailseite_21760.de.jsp" target="blank">Carsten Binnig<br>TU Darmstadt</a></td>
      <td style="border:none" bgcolor="white"><a href="http://web.cse.ohio-state.edu/~sun.397/" target="blank">Huan Sun<br>Ohio State University</a></td>
      <!-- <td style="border:none" bgcolor="white"><a href="https://hci.stanford.edu/~cagatay/" target="blank">Çağatay Demiralp<br>Sigma Computing</a></td>    -->
    </tr>
</table>


## Panelists

<table border="0" style="border:none; border-collapse:collapse; width: 100%; cellspacing:0; cellpadding:0" >
    <tr style="border:none"  align="left">
      <td style="border:none" width="16%"><a href="http://web.cse.ohio-state.edu/~sun.397/" target="blank"><img src="assets/hs.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>
      <td style="border:none" width="16%"><a href="https://ml.informatik.uni-freiburg.de/profile/hutter/" target="blank"><img src="assets/fh.png" width="150px" align="bottom" style="border-radius: 50%"></a></td>
      <td style="border:none" width="16%"><a href="http://blender.cs.illinois.edu/hengji.html" target="blank"><img src="assets/hj.png" width="150px" align="bottom" style="border-radius: 50%"></a></td>
    <td style="border:none" width="16%"><a href="https://eisenjulian.github.io/" target="blank"><img src="assets/je.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>
    <td style="border:none" width="16%"><a href="https://sebastianraschka.com/" target="blank"><img src="assets/sr.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>
    <td style="border:none" width="16%"><a href="http://www.phontron.com/" target="blank"><img src="assets/gn.jpg" width="150px" align="bottom" style="border-radius: 50%"></a></td>
    </tr>
    <tr style="border:none" align="left">
      <td style="border:none" bgcolor="white"><a href="http://web.cse.ohio-state.edu/~sun.397/" target="blank">Huan Sun (chair)<br>Ohio State University</a></td>
      <td style="border:none" bgcolor="white"><a href="https://ml.informatik.uni-freiburg.de/profile/hutter/" target="blank">Frank Hutter<br>University of Freiburg</a></td>
      <td style="border:none" bgcolor="white"><a href="http://blender.cs.illinois.edu/hengji.html" target="blank">Heng Ji<br>University of Illinois</a></td>
      <td style="border:none" bgcolor="white"><a href="https://eisenjulian.github.io/" target="blank">Julian Eisenschlos<br>Google Research</a></td>
      <td style="border:none" bgcolor="white"><a href="https://sebastianraschka.com/" target="blank">Sebastian Raschka<br>UW-Madison / Lightning AI</a></td>
      <td style="border:none" bgcolor="white"><a href="http://www.phontron.com/" target="blank">Graham Neubig<br>CMU</a></td>
    </tr>
</table>



## Schedule

The below schedule is preliminary. More details will follow soon.

| Time | Component | Speaker | Title |
| ---- | --------- | ------- | ----- |
| 08:30-08:45 | Opening notes |  |  |
| 08:45-09:30 | Keynote 1 | Alon Halevy<br> <details><summary>Bio</summary> Alon Halevy is a director at Meta’s Reality Labs Research, where he works on Human Value Alignment , the combination of neural and symbolic techniques for data management and on responsible personal information management. Prior to Meta, Alon was the CEO of Megagon Labs (2015-2018) and led the Structured Data Group at Google Research (2005-2015), where the team developed WebTables and Google Fusion Tables. From 1998 to 2005 he was a professor at the University of Washington, where he founded the database group. Alon is a founder of two startups, Nimble Technology and Transformic Inc. (acquired by Google in 2005). Alon co-authored two books: The Infinite Emotions of Coffee and Principles of Data Integration. In 2021 he received the Edgar F. Codd SIGMOD Innovations Award. Alon is a Fellow of the ACM and a recipient of the PECASE award and Sloan Fellowship. Together with his co-authors, he received VLDB 10-year best paper awards for the 2008 paper on WebTables and for the 1996 paper on the Information Manifold data integration system.</details> | Structured Data Inside and Out <br> <details><summary>Abstract</summary>WebTables contain high-quality data that is relevant to many queries on search engines. Since they are embedded inside web pages, understanding the semantics of tables requires analyzing the text surrounding them on the page. This talk will begin by recalling some of the early challenges we faced with the WebTables Project at Google. I will then turn to a different kind of challenge at the intersection of structured and unstructured data, where the structured data is outside and the unstructured data is inside. For example, when modeling a set of events in a person’s life (or history of an enterprise or a culture), each event is described in text and other media, but the event is also associated with structured data such as time and location. Answering questions over such collections of data requires leveraging the structure in the data appropriately. In the second half of the will discuss the motivations, challenges and partial solutions to dealing with structured data that is on the outside.</details> |
| 09:30-09:45 | Contributed talk | Aneta Koleva | <a href="assets/papers/analysis_of_the_attention_in_t.pdf" target="blank">Analysis of the Attention in Tabular Language Models</a> |
| 09:45-10:15 | Invited talk | Huan Sun <br> <details><summary>Bio</summary>Huan Sun is a tenured associate professor in the Department of Computer Science and Engineering at The Ohio State University. Before joining OSU, she was a visiting scientist at University of Washington, received a Ph.D. from UC Santa Barbara and a B.S. from University of Science and Technology of China. Her research interests lie in natural language processing, data mining, and artificial intelligence, with a focus on question answering, semantic parsing, conversational and interactive systems. Her research received the ACM SIGMOD Research Highlight Award and the Best Paper Award from the IEEE International Conference on Bioinformatics and Biomedicine (BIBM). She is a recipient of NSF CAREER Award, Google Research Scholar and Google Faculty Award, OSU Lumley Research Award, and SIGKDD Ph.D. Dissertation Runner-Up Award, among others. Her team TacoBot won third place in the first Alexa Prize TaskBot challenge in 2022 and was the only award-winning team in the US. Their TacoBot is currently deployed in Alexa.</details> | Self-supervised Pre-training on Tables<br><details><summary>Abstract</summary>Pre-training/fine-tuning paradigms have transformed the natural language processing field. For table-based tasks, however, their potential has been far less explored. In this talk, I will discuss the recent efforts led by my Ph.D. student Xiang Deng: (1) TURL, a pre-training/fine-tuning paradigm on relational Web tables, which benefits a wide range of tasks for table understanding (e.g., row population, relation extraction, entity linking). This work won the ACM SIGMOD Research Highlight Award in 2022. (2) StruG, a weakly supervised Structure-Grounded pretraining framework for text-to-SQL, which effectively learns to capture the text-table alignment essential for the task. At the time we tested our model on the Spider leaderboard in 2020, it was ranked 6th under the setting using DB content and 1st if without using DB content. (3) ReasonBERT, a pre-training method that augments language models for multi-step reasoning over hybrid contexts (textual and tabular). Among them, I will cover TURL in greater detail. Finally, I will conclude the talk with my thoughts about promising future directions. </details> |
| 10:15:10:30 | **Break** |  |  |
| 10:30-11:15 | Poster session 1 | - | <a href="/accepted-papers.html" target="blank">Accepted papers</a> |
| 11:15-11:45 | Invited talk | Carsten Binnig <br> <details><summary>Bio</summary> Carsten Binnig is a Full Professor in the Computer Science department at TU Darmstadt and a Visiting Researcher at the Google Systems Research Group. Carsten received his Ph.D. at the University of Heidelberg in 2008. Afterwards, he spent time as a postdoctoral researcher in the Systems Group at ETH Zurich and at SAP working on in-memory databases. Currently, his research focus is on the design of scalable data systems on modern hardware as well as machine learning for scalable data systems. His work has been awarded a Google Faculty Award, as well as multiple best paper and best demo awards. </details> | Pre-trained Models for Learned DBMS Components <br> <details><summary>Abstract</summary>Database management systems (DBMSs) are the backbone for managing large volumes of data efficiently and thus play a central role in business and science today. For providing high performance, many of the most complex DBMS components such as query optimizers or schedulers involve solving non-trivial problems such as query cost estimation. To tackle such problems, very recent work has outlined a new direction of so-called learned DBMS components where core parts of DBMSs are being replaced by machine learning (ML) models. While this line of work has shown to provide significant performance benefits for DBMS, a major drawback of the current so-called workload-driven learning approaches to enable learned DBMS components is that they cause a very high and repeated overhead for training data collection. Hence, in this talk, I will discuss a new direction of so-called zero-shot DBMS models which are pre-trained models that avoid the repeated training data collection overhead. As a concrete first step, we have realized a zero-shot cost model that can predict query execution cost which is a core DBMS task on an unseen database (i.e., a new set of tables with data) out of the box. Furthermore, I will also discuss other more recent results on how the general idea of zero-shot DBMS models can also be applied to other DBMS components as well or how it can even be applied even beyond DBMSs for other data systems.</details> |
| 11:45-12:00 | Contributed talk | Michał Pietruszka | <a href="assets/papers/stable_table_generation_framew.pdf" target="blank">STable: Table Generation Framework for <br> Encoder-Decoder Models</a> |
| 12:00-12:15 | Contributed talk | Roman Levin | <a href="assets/papers/transfer_learning_with_deep_ta.pdf" target="blank">Transfer Learning with Deep Tabular Models</a> |
| 12:15-13:30 | **Lunch** |  |  |
| 13:30-14:15 | Keynote 2 | Graham Neubig <br><details><summary>Bio</summary>Graham Neubig is an associate professor at the Language Technologies Institute of Carnegie Mellon University and CEO of Inspired Cognition. His research focuses on natural language processing, with a focus on multilingual NLP, natural language interfaces to computers, and machine learning methods for NLP system building and evaluation. His final goal is that every person in the world should be able to communicate with each-other, and with computers in their own language. He also contributes to making NLP research more accessible through open publishing of research papers, advanced NLP course materials and video lectures, and open-source software, all of which are available on his web site.</details> | Unsupervised Methods for Table and Schema Understanding<br><details><summary>Abstract</summary>In this talk I will discuss two methods that we have recently developed that allow for better understanding of tables. First, I will discuss OmniTab, a method for learning to represent tables using text- and table-based pre-training. Second, I will discuss a method for data augmentation that makes it possible to create pseudo-supervised training data for new database schemas.</details> |
| 14:15-14:30 | Contributed talk | David Vos | <a href="assets/papers/towards_parameter_efficient_au.pdf" target="blank">Towards Parameter-Efficient Automation of Data <br> Wrangling Tasks with Prefix-Tuning</a> |
| 14:30-14:45 | Contributed talk | Byung-Hak Kim | RegCLR: A Self-Supervised Framework for Tabular Representation Learning in the Wild |
| 14:45-15:00 | Contributed talk | Noah Hollmann | <a href="assets/papers/tabpfn_a_transformer_that_solv.pdf" target="blank">TabPFN: A Transformer That Solves Small Tabular <br> Classification Problems in a Second</a> |
| 15:00-15:15 | **Break** |  |  |
| 15:15-16:00 | Poster session 2 | - | <a href="/accepted-papers.html" target="blank">Accepted papers</a> |
| 16:00-16:30 | Invited talk | Xinyun Chen <br><details><summary>Bio</summary>Xinyun Chen is a senior research scientist in the Brain team of Google Research. She obtained her Ph.D. in Computer Science from University of California, Berkeley. Her research lies at the intersection of deep learning, programming languages, and security. Her recent research focuses on learning-based program synthesis and adversarial machine learning. She received the Facebook Fellowship in 2020, and Rising Stars in Machine Learning in 2021. Her work SpreadsheetCoder for spreadsheet formula prediction was integrated into Google Sheets, and she was part of the AlphaCode team when she interned at DeepMind.</details> | Program Synthesis from Semi-Structured Context <br> <details><summary>Abstract</summary>With the advancement of modern technologies, programming becomes ubiquitous not only among professional software developers, but also for general computer users. However, gaining programming expertise is time-consuming and challenging. Therefore, program synthesis has many applications, where the computer automatically synthesizes programs from user-written descriptions. In this talk, I will discuss my research on neural program synthesis from semi-structured context, where the synthesized program is executed on structured input for data processing and analysis. In particular, I will present my work SpreadsheetCoder for spreadsheet formula prediction, which was integrated into Google Sheets. Our work demonstrates that modeling the tabular structure and learning from multi-modal input is important for inferring user intents, especially when the program specifications are implicit and ambiguous.</details> |
| 16:30-17:30 | Panel | Huan Sun (chair), Frank Hutter, Julian Eisenschlos, <br>  Heng Ji, Sebastian Raschka, Graham Neubig  | Challenges, opportunities, and limitations of <br> Table Representation Learning (TBD) |
| 17:30-17:45 | Closing notes |  |  |
