---
layout: project
urltitle: "Table Representation Learning Workshop"
title: "Table Representation Learning Workshop"
categories: workshop, table representation learning, natural language, machine learning, neurips, 2023
permalink: /
bibtex: true
paper: true
acknowledgements: ""
---

<br />

<div class="row">
    <div class="col-xs-12">
        <p>
            <b>Tables are a promising modality for representation learning with too much application potential to ignore.</b>
            However, tables have long been overlooked despite their dominant presence in the data landscape, e.g. data management and analysis pipelines.
            The majority of datasets in Google Dataset Search, for example, resembles typical tabular file formats like CSVs.
            Similarly, the top-3 most-used database management systems are all intended for relational data (RDBMS).
            Representation learning over tables, possibly combined with other modalities such as text or SQL, has shown impressive performance for tasks like semantic parsing,
            question answering, table understanding, and data preparation. More recently, the pre-training paradigm was shown to be effective for tabular ML as well,
            while researchers also started exploring the impressive capabilities of LLMs for table encoding and data manipulation.
        </p>
        <p>
            The Table Representation Learning (TRL) workshop is the first in this emerging research area and
            concentrates on three main goals:
        </p>
        <p>
        <ul>
            <li> (1) <b>Motivate tables as a primary modality</b> for representation and generative learning and advance the area further. </li>
            <li> (2) <b>Showcase impactful applications of pretrained table models</b> and discussing future opportunities. </li>
            <li> (3) <b>Foster discussion and collaboration</b> across the ML, NLP and DB communities. </li>
        </ul>
        </p>
    </div>
    <div >
        <div class="col-xs-12">
            <!-- <div>
                Frequently Asked Questions: <a href="https://groups.google.com/g/table-representation-learning-workshop"
                    target="blank">FAQ</a>
            </div> -->
            <div>
                Public questions: <a class="u-email"
                    href="https://groups.google.com/g/table-representation-learning-workshop"target="blank">table-representation-learning-workshop@googlegroups.com</a>
            </div>
            <div>
                Private questions (or sponsor requests): <a class="u-email" href="mailto:m.hulsebos@uva.nl"  target="blank">m.hulsebos@uva.nl</a>
            </div>
            <div>
                Follow on Twitter:
                <i aria-hidden="true" class="fa fa-twitter"></i>
                <a class="menulink" href="https://twitter.com/TrlWorkshop" target="_blank">@TrlWorkshop</a>
            </div>
            <div>
                Submit:
                <b><a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL" target="blank">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL</a></b>
            </div>
        </div>
    </div>
</div>


    <!-- <p>With TRL we seek to address the following questions, among others:</p>

    <p>
    <ul>
        <li>How should tables be encoded to make learned Table Models generalize across tasks?</li>
        <li>Which pre-training objectives, architectures, fine-tuning and prompting strategies, work for tabular data?</li>
        <li>How should the varying formats, data types, and sizes of tables be handled?</li>
        <li>How and to what extent can LLMs be adapted towards tabular data tasks and what are their limitations?</li>
        <li>What tasks can existing neural table models accomplish well and what opportunities lie ahead?</li>
        <li>How do existing neural table models perform, what do they learn, where and how do they fall short?</li>
        <li>When and how should neural table models be updated in contexts where the underlying data source continuously evolves?</li>
    </ul>
    </p> -->

    <br>

    <hr />

    <!-- Schedule -->
    <!-- <div class="row" id="schedule">
        <div class="col-md-4 col-xs-12">
            <h2>Schedule</h2>
        </div>
        <div class="col-md-8 col-xs-12">
            <select id="timezone-select" class="form-control"></select>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <table class="table table-striped" id="schedule-table">
                <tbody>
                    <tr>
                        <th scope="row" data-time="08:00">08:00 AM</th>
                        <td>Virtual Poster Session 1</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="09:00">09:00 AM</th>
                        <td>Opening Remarks</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="09:10">09:10 AM</th>
                        <td>
                            Invited Speaker: Ellie Pavlick<br />
                            <span style="font-style:italic">Mechanistic Evidence of Structured Reasoning in LLMs</span>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="09:50">09:50 AM</th>
                        <td>
                            Invited Speaker: Noah Goodman<br />
                            <span style="font-style:italic">[Talk title forthcoming]</span>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="10:30">10:30 AM</th>
                        <td>Break 1</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="11:00">11:00 AM</th>
                        <td>
                            Oral Presentation: Li Zhang, Liam Dugan, Hainiu Xu and Chris Callison-burch<br />
                            <span style="font-style:italic">Exploring the Curious Case of Code Prompts</span>
                            <a data-toggle="collapse" href="#schedule-talk3" aria-cexpanded="false"
                                aria-controls="schedule-talk3">[Abstract]</a>
                            <div class="collapse" id="schedule-talk3">
                                Abstract: Recent work has shown that prompting language models with code-like
                                representations of natural language leads to performance improvements on structured
                                reasoning tasks. However, such tasks comprise only a small subset of all natural
                                language tasks. In our work, we seek to answer whether or not code-prompting is the
                                preferred way of interacting with language models in general. We compare code and text
                                prompts across three popular GPT models (davinci, code-davinci-002, and
                                text-davinci-002) on a broader selection of tasks (e.g., QA, sentiment, summarization)
                                and find that with few exceptions, code prompts do not consistently outperform text
                                prompts. Furthermore, we show that the style of code prompt has a large effect on
                                performance for some (but not all) tasks and that fine-tuning on text instructions leads
                                to better relative performance of code prompts.
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="11:10">11:10 AM</th>
                        <td>
                            Oral Presentation: Vanya Cohen and Raymond Mooney<br />
                            <span style="font-style:italic">Using Planning to Improve Semantic Parsing of Instructional
                                Texts</span>
                            <a data-toggle="collapse" href="#schedule-talk4" aria-cexpanded="false"
                                aria-controls="schedule-talk4">[Abstract]</a>
                            <div class="collapse" id="schedule-talk4">
                                Abstract: We develop a symbolic planning-based decoder to improve the few-shot semantic
                                parsing of instructional texts. The system takes long-form instructional texts as input
                                and produces sequences of actions in a formal language that enable execution of the
                                instructions. This task poses unique challenges since input texts may contain long
                                context dependencies and ambiguous and domain-specific language. Valid semantic parses
                                also require sequences of steps that constitute an executable plan. We build on recent
                                progress in semantic parsing by leveraging large language models to learn parsers from
                                small amounts of training data. During decoding, our method employs planning methods and
                                domain information to rank and correct candidate parses. To validate our method, we
                                evaluate on four domains: two household instruction-following domains and two cooking
                                recipe interpretation domains. We present results for few-shot semantic parsing using
                                leave-one-out cross-validation. We show that utilizing planning domain information
                                improves the quality of generated plans. Through ablations we also explore the effects
                                of our decoder design choices.
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="11:20">11:20 PM</th>
                        <td>In-Person Poster Session 1 / Virtual Poster Session 2 (See posters below)</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="12:20">12:20 PM</th>
                        <td>Lunch</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="13:30">13:30 PM</th>
                        <td>In-Person Poster Session 2 (See posters below)</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="14:30">14:30 PM</th>
                        <td>
                            Oral Presentation: Jinheon Baek, Alham Fikri Aji and Amir Saffari<br />
                            <span style="font-style:italic">Knowledge-Augmented Language Model Prompting for Zero-Shot
                                Knowledge Graph Question Answering</span>
                            <a data-toggle="collapse" href="#schedule-talk5" aria-cexpanded="false"
                                aria-controls="schedule-talk5">[Abstract]</a>
                            <div class="collapse" id="schedule-talk5">
                                Abstract: Large Language Models (LLMs) are capable of performing zero-shot closed-book
                                question answering tasks, based on their internal knowledge stored in parameters during
                                pre-training. However, such internalized knowledge might be insufficient and incorrect,
                                which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs
                                to update their knowledge is expensive. To this end, we propose to augment the knowledge
                                directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the
                                input question from the knowledge graph based on semantic similarities between the
                                question and its associated facts. After that, we prepend the retrieved facts to the
                                input question in the form of the prompt, which is then forwarded to LLMs to generate
                                the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING),
                                requires no model training, thus completely zero-shot. We validate the performance of
                                our KAPING framework on the knowledge graph question answering task, that aims to answer
                                the user's question based on facts over a knowledge graph, on which ours outperforms
                                relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various
                                sizes.
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="14:40">14:40 PM</th>
                        <td>
                            Oral Presentation: Michal Štefánik and Marek Kadlcik<br />
                            <span style="font-style:italic">Can In-context Learners Learn a Reasoning Concept from
                                Demonstrations?</span>
                            <a data-toggle="collapse" href="#schedule-6" aria-cexpanded="false"
                                aria-controls="schedule-6">[Abstract]</a>
                            <div class="collapse" id="schedule-6">
                                Abstract: Large language models show an emergent ability to learn a new task from a
                                small number of input-output demonstrations. However, recent work shows that in-context
                                learners largely rely on their pre-trained knowledge, such as the sentiment of the
                                labels, instead of finding new associations in the input. However, the commonly-used
                                few-shot evaluation settings using a random selection of in-context demonstrations can
                                not disentangle models' ability to learn a new skill from demonstrations, as most of the
                                randomly-selected demonstrations do not present relations informative for prediction
                                beyond exposing the new task distribution.
                                To disentangle models' in-context learning ability independent of models' memory, we
                                introduce a Conceptual few-shot learning method selecting the demonstrations sharing a
                                possibly-informative concept with the predicted sample. We extract a set of such
                                concepts from annotated explanations and measure how much can models benefit from
                                presenting these concepts in few-shot demonstrations.
                                We find that smaller models are more sensitive to the presented concepts. While some of
                                the models are able to benefit from concept-presenting demonstrations for each assessed
                                concept, we find that none of the assessed in-context learners can benefit from all
                                presented reasoning concepts consistently, leaving the in-context concept learning an
                                open challenge.
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="14:50">14:50 PM</th>
                        <td>
                            Invited Speaker: Peter Clark<br />
                            <span style="font-style:italic">The role of NL reasoning in the age of GPT</span>
                            <a data-toggle="collapse" href="#schedule-talk7" aria-cexpanded="false"
                                aria-controls="schedule-talk7">[Abstract]</a>
                            <a data-toggle="collapse" href="#speaker-bio-talk7" aria-cexpanded="false"
                                aria-controls="speaker-bio-talk7">[Speaker Bio]</a>
                            <div class="collapse" id="schedule-talk7">
                                Abstract: While the performance of new LLMs is stunning, it remains unclear how (or even
                                if) an answer follows from their latent "beliefs" about the world, or whether an LLM
                                even has a coherent internal belief system. In this talk I'll describe recent work we
                                have done to probe a model's beliefs, construct interpretable representations of how the
                                model's answers systematically follow from them, and how a broader system can identify
                                and repair inconsistencies that may exist among those beliefs. More generally, I'll
                                promote architectures in which interpretable, systematic NL reasoning and LLM-style
                                reasoning co-exist in a broader system, allowing both styles of reasoning to inform each
                                other, and paving the way for more interactive systems where users can probe, argue
                                with, learn from, and teach our future companions.
                            </div>
                            <div class="collapse" id="speaker-bio-talk7">
                                Peter Clark is a Senior Director and the interim CEO at the Allen Institute for AI
                                (AI2), and leads the Aristo Project. His work focuses on natural language processing,
                                machine reasoning, and world knowledge, and the interplay between these three areas.
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="15:30">15:30 PM</th>
                        <td>Break 2</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="16:00">16:00 PM</th>
                        <td>
                            Invited Speaker: Denny Zhou<br />
                            <span style="font-style:italic">Teach Language Models to Reason</span>
                            <a data-toggle="collapse" href="#schedule-talk8" aria-cexpanded="false"
                                aria-controls="schedule-talk8">[Abstract]</a>
                            <a data-toggle="collapse" href="#speaker-bio-talk8" aria-cexpanded="false"
                                aria-controls="speaker-bio-talk8">[Speaker Bio]</a>
                            <div class="collapse" id="schedule-talk8">
                                Over the past decades, the machine learning community has developed a multitude of
                                data-driven techniques aimed at enhancing learning efficiency. These include
                                semi-supervised learning, meta learning, active learning, transfer learning, and more.
                                However, none of these techniques have proven to be highly effective for real-world
                                natural language processing tasks. This shortcoming uncovers a fundamental flaw in
                                machine learning - the absence of reasoning. Humans often learn from just a few examples
                                because of their capacity to reason, as opposed to relying on data statistics. In this
                                talk, I will talk about the large language models (LLM) reasoning work that we
                                pioneered, and show that the techniques we developed can greatly narrow the gap between
                                human intelligence and machine learning: crushed SoTA in the literature while demanding
                                only a few annotated examples and no training. Our work was showcased at Google I/O 2022
                                by Google CEO Sundar Pichai.
                            </div>
                            <div class="collapse" id="speaker-bio-talk8">
                                Denny Zhou is a principal scientist / research director in Google DeepMind, where he is
                                the founder and current lead of the Reasoning Team. His primary research interest is
                                building and teaching large language models (LLMs) with an ambitious goal of attaining
                                human-level reasoning capabilities within these models. His team in Google has developed
                                chain-of-thought prompting, self-consistency decoding, least-to-most prompting,
                                instruction tuning (FLAN2), LLMs self-debugging and various investigations of emergent
                                properties of LLMs. He won Google Research Tech Impact Award in 2022.
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="16:40">16:40 PM</th>
                        <td>
                            Invited Speaker: Sarah Wiegreffe
                            <span style="font-style:italic">Two Views of Language Model Interpretability</span>
                            <a data-toggle="collapse" href="#schedule-talk9" aria-cexpanded="false"
                                aria-controls="schedule-talk9">[Abstract]</a>
                            <a data-toggle="collapse" href="#speaker-bio-talk9" aria-cexpanded="false"
                                aria-controls="speaker-bio-talk9">[Speaker Bio]</a>
                            <div class="collapse" id="schedule-talk9">
                                When generating text from language models (LMs), many prompting methods strive to
                                explain LM behavior by eliciting specifically-structured outputs (e.g, chain-of-thought
                                prompting). Relatedly, querying a model with specially-designed inputs and observing
                                output behavior is a longstanding and popular method in the NLP interpreter’s toolbox.
                                Prompting and querying approaches explain how LMs operate at a high-level (in natural
                                language) without attributing behaviors to any specific components of the network. A
                                separate line of work has investigated attributing or attempting to reconstruct model
                                behaviors at the model parameter or hidden representation-level, generally at a small
                                scale. While these two techniques often seem at odds in terms of their stated aims, they
                                collectively inform a large progression in our understanding of LMs in the past 2 years.
                                In this talk, I will give examples of both of these approaches, highlight their
                                similarities and differences, and discuss paths forward that leverage their combined
                                strengths.
                            </div>
                            <div class="collapse" id="speaker-bio-talk9">
                                Sarah Wiegreffe is a Young Investigator (postdoc) at the Allen Institute of AI, where
                                she is a member of the Aristo team. She also holds a courtesy appointment in the Allen
                                School at the University of Washington. Her research interests encompass
                                interpretability + explainability of NLP models, with a focus on the faithfulness of
                                generated text to internal LM prediction mechanisms and the utility of model-generated
                                textual explanations to humans. She received her PhD in 2022 from Georgia Tech, advised
                                by Mark Riedl. She also received an M.S. in Computer Science (2020) and B.S. in Data
                                Science (2017) from Georgia Tech and the College of Charleston, respectively. Outside of
                                work, she enjoys rock climbing, cooking, and exploring Seattle.
                            </div>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div> -->

    <!-- <hr /> -->

        <!-- Speakers -->
        <div class="row" id="speakers">
            <div class="col-xs-12">
                <h2>Program</h2>

                TRL is again entirely in-person, and will this year feature 2 longer poster sessions and more (spotlight) talks on contributed work.<br>
                We also host a few exciting invited talks on established research in this emerging area, and a panel discussion.<br>

                <br>

                <h3>Invited Speakers</h3>
                <br>
                <div class="row">
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://wenhuchen.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/wc.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://wenhuchen.github.io/" target="blank">Wenhu Chen</a>
                            <h6>University of Waterloo,<br>Google DeepMind, Vector Institute</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://ml.informatik.uni-freiburg.de/profile/hutter/" target="blank">
                            <img class="people-pic" src="/assets/people/fh.png">
                        </a>
                        <div class="people-name">
                            <a href="https://ml.informatik.uni-freiburg.de/profile/hutter/" target="blank">Frank Hutter</a>
                            <h6>University of Freiburg</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://arorasimran.com/" target="blank">
                            <img class="people-pic" src="/assets/people/sa.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://arorasimran.com/" target="blank">Simran Arora</a>
                            <h6>Stanford University</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://itrummer.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/it.jpeg">
                        </a>
                        <div class="people-name">
                            <a href="https://itrummer.github.io/" target="blank">Immanuel Trummer</a>
                            <h6>Cornell University</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://taoyds.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/ty.jpeg">
                        </a>
                        <div class="people-name">
                            <a href="https://taoyds.github.io/" target="blank">Tao Yu</a>
                            <h6>University of Hong Kong</h6>
                        </div>
                        <br>
                    </div>
                </div>
    
                <h3>Panelists</h3>
                TBC
            </div>
        </div>
    <hr />

    <div>
        <div class="row" id="cfp">
            <div class="col-xs-12">
                <h2>Call for Papers</h2>
            </div>
        </div>
    </div>
    <br>
    <div class="row" id="dates">
        <div class="col-xs-12">
            <h3>Important Dates</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <br>
            <table class="table table-striped">
                <tbody>
                    <tr>
                        <td>Submission Open</td>
                        <td>August 10, 2023</td>
                    </tr>
                    <tr>
                        <td>Submission Deadline</td>
                        <td><b>October <s>2</s> <b>4</b>, 2023</b> (11:59PM AoE)</td>
                    </tr>
                    <tr>
                        <td>Notifications</td>
                        <td><b>October 27, 2023</b> (11:59PM AoE)</td>
                    </tr>
                    <tr>
                        <td>Workshop Date</td>
                        <td>December 15, 2023</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <h3>Scope</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">

            <p>We invite submissions on representation and generative learning over tables, related to any of the following topics:
            </p>

            <ul>
                <li><b>Representation Learning over tables  </b> which can be structured as well as semi-structured, and extend to full databases. 
                    Example contributions are new model architectures, data encoding techniques, pre-training,
                    fine-tuning, and prompting strategies, multi-task learning, etc.
                </li>
                <li><b>Generative Learning and LLMs  </b>  for structured data and interfaces to structured data
                    (e.g. queries, analysis).
                </li>
                <li><b>Multimodal learning  </b>
                    where tables are jointly embedded with, for example, natural language, code (e.g. SQL),
                    knowledge bases, visualizations/images.
                </li>
                <li><b>Downstream Applications  </b> of table representations for tasks like data preparation
                    (e.g. data cleaning, validation, integration, cataloging, feature engineering),
                    retrieval (e.g. search, fact-checking/QA, KG construction),
                    analysis (e.g. summarization, visualization, and query recommendation), and (end-to-end) machine learning.
                </li>
                <li><b>Upstream Applications  </b> of table representation models for optimizing table parsers/extraction
                    (from documents, spreadsheets, presentations), storage (e.g. compression, indexing)
                    and query processing e.g. query optimization
                </li>
                <li><b>Production challenges  </b>of table representation models.
                    Work addressing the challenges of maintaining and managing TRL models in fast evolving contexts,
                    e.g. data updating, error correction, and monitoring, and other industry challenges such as privacy, personalization performance, etc.
                </li>
                <li><b>Domain-specific challenges  </b>for learned table models often arise in domains such as enterprise,
                    finance, medical, law. These challenges pertain to table content, table structure, privacy, security
                    limitations, and other factors that necessitate tailored solutions.
                </li>
                <li><b>Benchmarks and analyses  </b>of table representation models, including the utility of language models as base models
                    versus alternatives and robustness regarding large, messy, heterogeneous, or complex tables.
                </li>
                <li><b>Others: </b> Formalization, surveys, datasets, visions, and reflections to structure and guide future
                    research.
                </li>
            </ul>
        </div>
    </div>

    <hr />

    <div class="row" id="guidelines">
        <div class="col-xs-12">
            <h2>Submission Guidelines</h2>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <h3>Submission link</h3>
            Submit your (anonymized) paper through OpenReview at: <b><a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL" target="blank">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL</a></b><br>
            Please be aware that accepted papers are expected to be presented at the workshop in-person.
        </div>

        <div class="col-xs-12">
            <br>
            <h3>Formatting guidelines</h3>
            The workshop accepts regular research papers and industrial papers of the following types:
            <ul>
                <div class="col-xs-12">
                    <li>Short paper: 4 pages + references.</li>
                    <li>Regular paper: 8 pages + references.</li>
                </div>
            </ul>
            <br>
            <br>
            Submissions should be anonymized and follow the NeurIPS style files, but can exclude the checklist.
            Non-anonymous preprints are no problem, and artifacts do not have to be anonymized. Just submitting the paper without author names/affiliations is sufficient.
            Supplementary material, if any, may be added in the appendix.
            The footer of accepted papers should state “Table Representation Learning Workshop at NeurIPS 2023”. We expect authors to adopt an inclusive and diverse writing style.
            The <a href="https://dbdni.github.io/pages/inclusivewriting.html" target="blank">“Diversity and Inclusion in Writing”</a> guide by the DE&I in DB Conferences effort is a good resource.
        </div>
        
        <div class="col-xs-12">
            <br>
            <h3>Review process</h3>
            Papers will receive light reviews in a double-anonymous manner.
            All accepted submissions will be published on the website and made public on OpenReview but the workshop is non-archival (i.e. without proceedings).
        </div>

        <div class="col-xs-12">
            <br>
            <h3>Novelty and conflicts</h3>
            The workshop does not accept submissions that have previously been published at NeurIPS or other machine learning venues.
            However, we do welcome submissions that have been published in, for example, data management or natural language processing venues.
            We rely on OpenReview for handling conflicts, so please ensure that the conflicts in every author's OpenReview profile are complete, in particular, with respect to the organization and program committees.
            <br>
        </div>
    </div>

    <hr />

    <!-- Organizers -->
    <div class="row" id="organization">
        <div class="col-xs-12">
            <h2>Organization</h2>
        </div>

    <div class="col-xs-12">
        <h3>Workshop Chairs</h3>
        <br>
        <div class="col-xs-6 col-lg-3">
            <a href="https://www.madelonhulsebos.com/" target="blank">
                <img class="people-pic" src="assets/people/mh.jpg">
            </a>
            <div class="people-name">
                <a href="https://www.madelonhulsebos.com/" target="blank">Madelon Hulsebos</a>
                <h6>University of Amsterdam</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://www.microsoft.com/en-us/research/people/hadong" target="blank">
                <img class="people-pic" src="assets/people/hd.jpg">
            </a>
            <div class="people-name">
                <a href="https://www.microsoft.com/en-us/research/people/hadong" target="blank">Haoyu Dong</a>
                <h6>Microsoft Research Asia</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://bojan.ninja" target="blank">
                <img class="people-pic" src="assets/people/bk.jpg">
            </a>
            <div class="people-name">
                <a href="https://bojan.ninja" target="blank">Bojan Karlas</a>
                <h6>Harvard</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://cs.stanford.edu/people/lorr1" target="blank">
                <img class="people-pic" src="assets/people/lo.jpg">
            </a>
            <div class="people-name">
                <a href="https://cs.stanford.edu/people/lorr1" target="blank">Laurel Orr</a>
                <h6>Numbers Station AI</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://research.google/people/PengchengYin/" target="blank">
                <img class="people-pic" src="assets/people/py.jpg">
            </a>
            <div class="people-name">
                <a href="https://research.google/people/PengchengYin/" target="blank">Pengcheng Yin</a>
                <h6>Google DeepMind</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://gael-varoquaux.info/" target="blank">
                <img class="people-pic" src="assets/people/gv.jpg">
            </a>
            <div class="people-name">
                <a href="https://gael-varoquaux.info/" target="blank">Gaël Varoquaux</a>
                <h6>INRIA</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://siviltaram.github.io/" target="blank">
                <img class="people-pic" src="assets/people/ql.png">
            </a>
            <div class="people-name">
                <a href="https://siviltaram.github.io/" target="blank">Qian Liu</a>
                <h6>Sea AI Lab</h6>
                <br>
            </div>
        </div>
    </div>

    <div class="col-xs-12">
        <h3>Program Committee</h3>
        Vadim Borisov,	University of Tuebingen<br>
        Paul Groth,	University of Amsterdam<br>
        Wensheng Dou,	Institute of Software Chinese Academy of Sciences<br>
        Hiroshi Iida,	The University of Tokyo<br>
        Sharad Chitlangia,	Amazon<br>
        Jaehyun Nam,	KAIST<br>
        Jinyang Li,	The University of Hong Kong<br>
        Gerardo Vitagliano,	Hasso Plattner Institute<br>
        Rajat Agarwal,	Amazon<br>
        Micah Goldblum,	New York University<br>
        Yury Gorishniy,	Yandex Research<br>
        Roman Levin,	Amazon<br>
        Bhavesh Neekhra,	Ashoka University<br>
        Binyuan Hui,	Alibaba Group<br>
        Yu Su,	Ohio State University<br>
        Sebastian Schelter,	University of Amsterdam<br>
        Lei Cao,	University of Arizona / MIT CSAIL<br>
        Qingping Yang,	University of Chinese Academy of Sciences<br>
        Matteo Interlandi,	Microsoft<br>
        Tianji Cong,	University of Michigan<br>
        Xiang Deng,	Google<br>
        Beliz Gunel,	Google<br>
        Qian Liu,	Sea AI Lab<br>
        Xinyun Chen,	Google<br>
        Shuaichen Chang,	Ohio State University<br>
        Zhoujun Cheng,	Shanghai Jiaotong University<br>
        Vivek Gupta,	University of Pennsylvania<br>
        Roee Shraga,	Worcester Polytechnic Institute<br>
        Yi Zhang,	AWS AI Labs<br>
        Xi Rao,	ETH Zurich<br>
        Liane Vogel, Technical University of Darmstadt<br>
        Aneta Koleva, University of Munich / Siemens<br>
        Ivan Rubachev, HSE University / Yandex<br>
        Meghana Moorthy Bhat, Salesforce Research<br>
        José Cambronero, Microsoft<br>
        Till Döhmen, University of Amsterdam<br>
        Noah Hollman, Charité Berlin / University of Freiburg<br>
        Julian Martin Eisenschlos, Google<br>
        Paolo Papotti, Eurecom<br>
        Zhiruo Wang, Carnegie Mellon University<br>
        Mukul Singh, Microsoft<br>
        Zezhou Huang, Columbia University<br>
        Carsten Binnig, TU Darmstadt<br>
        Linyong Nan, Yale<br>
        Shuo Zhang, Bloomberg<br>
        Alejandro Sierra Múnera, Hasso Plattner Institute<br>
        Qian Liu, Sea AI Labs<br>
        Haoyu Dong, Microsoft<br><br>
    </div>


    <!-- <div class="col-xs-12">
        <h3>Review for TRL?</h3>
        We are compiling a PC for light reviewing of submissions to TRL 2023 (at most 3 reviews per reviewer).<br>
        We invite researchers with expertise relevant to TRL to express their interest in reviewing through the below form.<br>
        Your time and expertise is much appreciated!
        <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSdjlbY-590PzDNBtOlUlCzxD_Uhbkmq389pRieW7L5B7F31_g/viewform?embedded=true" width="680" height="320" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
        <br>
    </div> -->

    </div>

    <hr />

    <div class="row" id="accepted-papers">
        <div class="col-xs-12">
            <h2>Accepted Papers</h2>
            <!-- <p>Note: 2 additional papers were accepted but are not listed here because of an anonymity period.</p> -->
        </div>
    </div>

    <br/>

    <div>
        <h3>2023</h3>

        <br/>

        <b>TBC</b>
    </div>

    <br/>
    <br/>

    <div>
        <h3>2022</h3>

        <br/>

        <h4>Oral</h4>

        <ul class="paper-list">
        <li>
            <a class="paper-title" href="../../assets/papers/analysis_of_the_attention_in_t.pdf" target="_blank">Analysis of the Attention in
                Tabular
                Language Models</a> &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993349" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
            <br>
            <span class="paper-authors">Aneta Koleva, Martin Ringsquandl, Volker Tresp</span>
            <br>
        </li>
        </li>

        <li> <a class="paper-title" href="assets/papers/transfer_learning_with_deep_ta.pdf" target="_blank">Transfer Learning with Deep
                Tabular
                Models</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993348" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
            <br>
            <span class="paper-authors">Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C. Bayan
                Bruss,
                Tom
                Goldstein, Andrew Gordon Wilson, Micah Goldblum</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stable_table_generation_framew.pdf" target="_blank">STable: Table Generation
                Framework
                for
                Encoder-Decoder Models</a>
                    &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993352" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>

                    <br>
            <span class="paper-authors">Michał Pietruszka, Michał Turski, Łukasz Borchmann, Tomasz Dwojak, Gabriela
                Pałka,
                Karolina Szyndler, Dawid Jurkiewicz, Łukasz Garncarek</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/tabpfn_a_transformer_that_solv.pdf" target="_blank">TabPFN: A Transformer That
                Solves
                Small
                Tabular Classification Problems in a Second</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993350" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">Noah Hollmann, Samuel Müller, Katharina Eggensperger, Frank Hutter</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/towards_parameter_efficient_au.pdf" target="_blank">Towards Parameter-Efficient
                Automation
                of Data Wrangling Tasks with Prefix-Tuning</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993351" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">David Vos, Till Döhmen, Sebastian Schelter</span>
        </li>

        <li> <a class="paper-title" href="https://openreview.net/forum?id=7q_-aEdnGZw" target="_blank">RegCLR: A Self-Supervised Framework
                for
                Tabular Representation Learning in the Wild</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38996604" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">Weiyao Wang, Byung-Hak Kim, Varun Ganapathi</span>
        </li>
        </ul>

        <br/>

        <h4>Poster</h4>
        
        <br/>

        <ul class="paper-list">
        <li> <a class="paper-title" href="assets/papers/saint_improved_neural_networks.pdf" target="_blank">SAINT: Improved Neural Networks
                for
                Tabular Data via Row Attention and Contrastive Pre-Training</a><br>
            <span class="paper-authors">Gowthami Somepalli, Avi Schwarzschild, Micah Goldblum, C. Bayan Bruss, Tom
                Goldstein</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/generic_entity_resolution_mode.pdf" target="_blank">Generic Entity Resolution
                Models</a><br>
            <span class="paper-authors">Jiawei Tang, Yifei Zuo, Lei Cao, Samuel Madden</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/towards_foundation_models_for_.pdf" target="_blank">Towards Foundation Models for
                Relational
                Databases</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=GyeGQGmTv30" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                <br>
            <span class="paper-authors">Liane Vogel, Benjamin Hilprecht, Carsten Binnig</span>
        </li>


        <li> <a class="paper-title" href="assets/papers/diffusion_models_for_missing_v.pdf" target="_blank">Diffusion models for missing
                value
                imputation in tabular data</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=URlh7KJfXzM" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                <br>
            <span class="paper-authors">Shuhan Zheng, Nontawat Charoenphakdee</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stab_self_supervised_learning_.pdf" target="_blank">STab: Self-supervised Learning
                for
                Tabular Data</a><br>
            <span class="paper-authors">Ehsan Hajiramezanali, Max W Shen, Gabriele Scalia, Nathaniel Lee Diamant</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/caspr_customer_activity_sequen.pdf" target="_blank">CASPR: Customer Activity
                Sequence
                based
                Prediction and Representation</a><br>
            <span class="paper-authors">Damian Konrad Kowalczyk, Pin-Jung Chen, Sahil Bhatnagar</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/conditional_contrastive_networ.pdf" target="_blank">Conditional Contrastive
                Networks</a><br>
            <span class="paper-authors">Emily Mu, John Guttag</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/self_supervised_representation.pdf" target="_blank">Self-supervised Representation
                Learning
                Across Sequential and Tabular Features Using Transformers</a><br>
            <span class="paper-authors">Rajat Agarwal, Anand Muralidhar, Agniva Som, Hemant Kowshik</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/the_need_for_tabular_represent.pdf" target="_blank">The Need for Tabular
                Representation
                Learning: An Industry Perspective</a><br>
            <span class="paper-authors">Joyce Cahoon, Alexandra Savelieva, Andreas C Mueller, Avrilia Floratou, Carlo
                Curino,
                Hiren Patel, Jordan Henkel, Markus Weimer, Roman Batoukov, Shaleen Deep, Venkatesh Emani, Richard
                Wydrowski,
                Nellie Gustafsson</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stunt_few_shot_tabular_learnin.pdf" target="_blank">STUNT: Few-shot Tabular Learning
                with
                <span class="paper-authors">Self-generated Tasks from Unlabeled Tables</a><br>
            Jaehyun Nam, Jihoon Tack, Kyungmin Lee, Hankook Lee, Jinwoo Shin</span></li>

        <li> <a class="paper-title" href="assets/papers/tabular_data_generation_can_we.pdf" target="_blank">Tabular Data Generation: Can We
                Fool
                XGBoost?</a>
            <br>
            <span class="paper-authors">EL Hacen Zein, Tanguy Urvoy</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/sima_federating_data_silos_usi.pdf" target="_blank">SiMa: Federating Data Silos using
                GNNs</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=y4ZOobI1v2w" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                    <br>
            <span class="paper-authors">Christos Koutras, Rihan Hai, Kyriakos Psarakis, Marios Fragkoulis, Asterios
                Katsifodimos

        <li> <a class="paper-title" href="assets/papers/self_supervised_pre_training_f.pdf" target="_blank">Self Supervised Pre-training for
                Large Scale Tabular Data</a><br>
            <span class="paper-authors">Sharad Chitlangia, Anand Muralidhar, Rajat Agarwal</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/rotar_efficient_row_based_tabl.pdf" target="_blank">RoTaR: Efficient Row-Based Table
                Representation Learning via Teacher-Student Training</a><br>
            <span class="paper-authors">Zui Chen, Lei Cao, Samuel Madden</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/mapqa_a_dataset_for_question_a.pdf" target="_blank">MapQA: A Dataset for Question
                Answering on Choropleth Maps</a><br>
            <span class="paper-authors">Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, Ningchuan
                Xiao</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/met_masked_encoding_for_tabula.pdf" target="_blank">MET: Masked Encoding for Tabular
                Data</a><br>
            <span class="paper-authors">Kushal Alpesh Majmundar, Sachin Goyal, Praneeth Netrapalli, Prateek Jain</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/active_learning_with_tabular_l.pdf" target="_blank">Active Learning with Table
                Language
                Models</a><br>
            <span class="paper-authors">Martin Ringsquandl, Aneta Koleva</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/structural_embedding_of_data_f.pdf" target="_blank">Structural Embedding of Data
                Files
                with MAGRITTE</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=_seBQIBzFoI" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                    <br>
            <span class="paper-authors">Gerardo Vitagliano, Mazhar Hameed, Felix Naumann</span>
        </li>
        </ul>

    </div>