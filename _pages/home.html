---
layout: project
urltitle: "Table Representation Learning Workshop"
title: "Table Representation Learning Workshop"
categories: workshop, table representation learning, natural language, machine learning, neurips, 2023
permalink: /
bibtex: true
paper: true
acknowledgements: ""
---

<br />

<div class="row">
    <div class="col-xs-12">
        <p>
            <b>Tables are a promising modality for representation learning with too much application potential to ignore.</b>
            However, tables have long been overlooked despite their dominant presence in the data landscape, e.g. data management and analysis pipelines.
            The majority of datasets in Google Dataset Search, for example, resembles typical tabular file formats like CSVs.
            Similarly, the top-3 most-used database management systems are all intended for relational data (RDBMS).
            Representation learning over tables, possibly combined with other modalities such as text or SQL, has shown impressive performance for tasks like semantic parsing,
            question answering, table understanding, and data preparation. More recently, the pre-training paradigm was shown to be effective for tabular ML as well,
            while researchers also started exploring the impressive capabilities of LLMs for table encoding and data manipulation.
        </p>
        <p>
            The Table Representation Learning (TRL) workshop is the first in this emerging research area and
            concentrates on three main goals:
        </p>
        <p>
        <ul>
            <li> (1) <b>Motivate tables as a primary modality</b> for representation and generative learning and advance the area further. </li>
            <li> (2) <b>Showcase impactful applications of pretrained table models</b> and discussing future opportunities. </li>
            <li> (3) <b>Foster discussion and collaboration</b> across the ML, NLP and DB communities. </li>
        </ul>
        </p>
    </div>
    <div >
        <div class="col-xs-12">
            <!-- <div>
                Frequently Asked Questions: <a href="https://groups.google.com/g/table-representation-learning-workshop"
                    target="blank">FAQ</a>
            </div> -->
            <div>
                Public questions: <a class="u-email"
                    href="https://groups.google.com/g/table-representation-learning-workshop"target="blank">table-representation-learning-workshop@googlegroups.com</a>
            </div>
            <div>
                Private questions (or sponsor requests): <a class="u-email" href="mailto:m.hulsebos@uva.nl"  target="blank">m.hulsebos@uva.nl</a>
            </div>
            <div>
                Follow on Twitter:
                <i aria-hidden="true" class="fa fa-twitter"></i>
                <a class="menulink" href="https://twitter.com/TrlWorkshop" target="_blank">@TrlWorkshop</a>
            </div>
            <div>
                Submit:
                <b><a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL" target="blank">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL</a></b>
            </div>
        </div>
    </div>
</div>


    <!-- <p>With TRL we seek to address the following questions, among others:</p>

    <p>
    <ul>
        <li>How should tables be encoded to make learned Table Models generalize across tasks?</li>
        <li>Which pre-training objectives, architectures, fine-tuning and prompting strategies, work for tabular data?</li>
        <li>How should the varying formats, data types, and sizes of tables be handled?</li>
        <li>How and to what extent can LLMs be adapted towards tabular data tasks and what are their limitations?</li>
        <li>What tasks can existing neural table models accomplish well and what opportunities lie ahead?</li>
        <li>How do existing neural table models perform, what do they learn, where and how do they fall short?</li>
        <li>When and how should neural table models be updated in contexts where the underlying data source continuously evolves?</li>
    </ul>
    </p> -->

    <br>

    <hr />

    <!-- Schedule -->
    <div class="row" id="schedule">
        <div class="col-md-4 col-xs-12">
            <h2>Schedule</h2>
        </div>
        <div class="col-md-8 col-xs-12">
            <select id="timezone-select" class="form-control"></select>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <table class="table table-striped" id="schedule-table">
                <tbody>
                    <tr>
                        <th scope="row" data-time="06:30">06:30 AM</th>
                        <td>Opening Notes</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="06:45">06:45 AM</th>
                        <td>
                            Invited Speaker: Simran Arora<br />
                            <span style="font-style:italic">
                                [Talk title forthcoming]
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="07:15">07:15 AM</th>
                        <td>Spotlight Talks: Session 1</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="08:00">08:00 AM</th>
                        <td>Coffee Break / Poster Setup</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="08:30">08:30 AM</th>
                        <td>Poster: Session 1</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="09:00">09:00 AM</th>
                        <td>
                            Invited Speaker: Frank Hutter<br />
                            <span style="font-style:italic">[Talk title forthcoming]</span>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="09:30">09:30 AM</th>
                        <td>Spotlight Talks: Session 2</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="10:00">10:00 AM</th>
                        <td>Lunch Break</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="11:30">11:30 AM</th>
                        <td>
                            Invited Speaker: Immanuel Trummer<br />
                            <span style="font-style:italic">Next-Generation Data Management with Large Language Models</span>
                            <a data-toggle="collapse" href="#schedule-talk3" aria-cexpanded="false"
                                aria-controls="schedule-talk3">[Abstract]</a>
                            <div class="collapse" id="schedule-talk3">
                                Abstract: The past years have been marked by several breakthrough results in the domain of generative AI, culminating in the rise of tools like ChatGPT, able to solve a variety of language-related tasks without specialized training. In this talk, I discuss several recent research projects at Cornell, exploiting large language models to enhance relational database management systems. These projects cover applications of language models in the database interface, enabling users to specify high-level analysis goals for fully automated end-to-end analysis, as well as applications in the backend, using language models to extract useful information for data profiling and database tuning from text documents.
                            </div>
                            <a data-toggle="collapse" href="#speaker-bio-talk3" aria-cexpanded="false"
                            aria-controls="speaker-bio-talk3">[Speaker Bio]</a>
                            <div class="collapse" id="schedule-bio-talk3">
                                Immanuel Trummer is an assistant professor at Cornell University and a member of the Cornell Database Group. His papers were selected for “Best of VLDB”, “Best of SIGMOD”, for the ACM SIGMOD Research Highlight Award, and for publication in CACM as CACM Research Highlight. His online lecture introducing students to database topics collected over a million views. He received the NSF CAREER Award and multiple Google Faculty Research Awards.
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="12:00">12:00 PM</th>
                        <td>Spotlight Talks: Session 3</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="12:30">12:30 PM</th>
                        <td>
                            Invited Speaker: Tao Yu<br />
                            <span style="font-style:italic">[Talk title forthcoming]</span>
                            <!-- <a data-toggle="collapse" href="#schedule-talk3" aria-cexpanded="false"
                                aria-controls="schedule-talk3">[Abstract]</a>
                            <div class="collapse" id="schedule-talk3">
                                Abstract: The past years have been marked by several breakthrough results in the domain of generative AI, culminating in the rise of tools like ChatGPT, able to solve a variety of language-related tasks without specialized training. In this talk, I discuss several recent research projects at Cornell, exploiting large language models to enhance relational database management systems. These projects cover applications of language models in the database interface, enabling users to specify high-level analysis goals for fully automated end-to-end analysis, as well as applications in the backend, using language models to extract useful information for data profiling and database tuning from text documents.
                            </div>
                            <a data-toggle="collapse" href="#speaker-bio-talk3" aria-cexpanded="false"
                            aria-controls="speaker-bio-talk3">[Speaker Bio]</a>
                            <div class="collapse" id="schedule-bio-talk3">
                                Immanuel Trummer is an assistant professor at Cornell University and a member of the Cornell Database Group. His papers were selected for “Best of VLDB”, “Best of SIGMOD”, for the ACM SIGMOD Research Highlight Award, and for publication in CACM as CACM Research Highlight. His online lecture introducing students to database topics collected over a million views. He received the NSF CAREER Award and multiple Google Faculty Research Awards.
                            </div> -->
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="13:00">01:00 PM</th>
                        <td>Coffee Break / Poster Setup</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="13:30">01:30 PM</th>
                        <td>Poster: Session 2</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="14:00">02:00 PM</th>
                        <td>
                            Invited Speaker: Wenhu Chen<br />
                            <span style="font-style:italic">[Talk title forthcoming]</span>
                            <a data-toggle="collapse" href="#schedule-talk5" aria-cexpanded="false"
                                aria-controls="schedule-talk5">[Abstract]</a>
                            <div class="collapse" id="schedule-talk5">
                                Abstract: Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack comprehensive studies examining whether LLMs can truly comprehend such data. In this talk, I will cover different ways to utilize LLMs to interface with tables. One approach is to feed the whole table as a sequence to LLMs for reasoning. In this direction, we will talk about the recent paper GPT4Table to summarize the lessons learned in different table linearization strategies, including table input format, content order, role prompting, and partition marks. The other approach is to use tools like SQL or other language to interface with table for data access without feeding the entire table. LLMs will work as a reasoner to derive the answer based on the interfaced results from the table.
                            </div>
                            <a data-toggle="collapse" href="#speaker-bio-talk5" aria-cexpanded="false"
                            aria-controls="speaker-bio-talk5">[Speaker Bio]</a>
                            <div class="collapse" id="schedule-bio-talk5">
                                Wenhu Chen has been an assistant professor at Computer Science Department in University of Waterloo and Vector Institute since 2022. He obtained Canada CIFAR AI Chair Award in 2022. He also works for Google Deepmind as a part-time research scientist since 2021. Before that, he obtained his PhD from the University of California, Santa Barbara under the supervision of William Wang and Xifeng Yan. His research interest lies in natural language processing, deep learning and multimodal learning. He aims to design models to handle complex reasoning scenarios like math problem-solving, structure knowledge grounding, etc. He is also interested in building more powerful multimodal models to bridge different modalities. He received the Area Chair Award in AACL-IJCNLP 2023, the Best Paper Honorable Mention in WACV 2021, and the UCSB CS Outstanding Dissertation Award in 2021.
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="14:30">02:30 PM</th>
                        <td>Panel - TBA</td>
                    </tr>
                    <tr>
                        <th scope="row" data-time="15:15">03:15 PM</th>
                        <td>Closing Notes</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <!-- <hr /> -->

        <!-- Speakers -->
        <div class="row" id="speakers">
            <div class="col-xs-12">
                <h2>Program</h2>

                TRL is again entirely in-person, and will this year feature 2 longer poster sessions and more (spotlight) talks on contributed work.<br>
                We also host a few exciting invited talks on established research in this emerging area, and a panel discussion.<br>

                <br>

                <h3>Invited Speakers</h3>
                <br>
                <div class="row">
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://wenhuchen.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/wc.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://wenhuchen.github.io/" target="blank">Wenhu Chen</a>
                            <h6>University of Waterloo,<br>Google DeepMind, Vector Institute</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://ml.informatik.uni-freiburg.de/profile/hutter/" target="blank">
                            <img class="people-pic" src="/assets/people/fh.png">
                        </a>
                        <div class="people-name">
                            <a href="https://ml.informatik.uni-freiburg.de/profile/hutter/" target="blank">Frank Hutter</a>
                            <h6>University of Freiburg</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://arorasimran.com/" target="blank">
                            <img class="people-pic" src="/assets/people/sa.jpg">
                        </a>
                        <div class="people-name">
                            <a href="https://arorasimran.com/" target="blank">Simran Arora</a>
                            <h6>Stanford University</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://itrummer.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/it.jpeg">
                        </a>
                        <div class="people-name">
                            <a href="https://itrummer.github.io/" target="blank">Immanuel Trummer</a>
                            <h6>Cornell University</h6>
                        </div>
                    </div>
                    <div class="col-xs-6 col-lg-2">
                        <a href="https://taoyds.github.io/" target="blank">
                            <img class="people-pic" src="/assets/people/ty.jpeg">
                        </a>
                        <div class="people-name">
                            <a href="https://taoyds.github.io/" target="blank">Tao Yu</a>
                            <h6>University of Hong Kong</h6>
                        </div>
                        <br>
                    </div>
                </div>
    
                <h3>Panelists</h3>
                TBC
            </div>
        </div>
    <hr />

    <div>
        <div class="row" id="cfp">
            <div class="col-xs-12">
                <h2>Call for Papers</h2>
            </div>
        </div>
    </div>
    <br>
    <div class="row" id="dates">
        <div class="col-xs-12">
            <h3>Important Dates</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <br>
            <table class="table table-striped">
                <tbody>
                    <tr>
                        <td>Submission Open</td>
                        <td>August 10, 2023</td>
                    </tr>
                    <tr>
                        <td>Submission Deadline</td>
                        <td><s><b>October <b>4</b>, 2023</b> (11:59PM AoE)</s></td>
                    </tr>
                    <tr>
                        <td>Notifications</td>
                        <td><s><b>October 27, 2023</b> (11:59PM AoE)</s></td>
                    </tr>
                    <tr>
                        <td style="color: red">Camera-ready</td>
                        <td style="color: red">November 15, 2023 (11:59PM AoE)</td>
                    </tr>
                    <tr>
                        <td style="color: red">Slides for spotlight talks</td>
                        <td style="color: red">November 28, 2023 (11:59PM AoE)</td>
                    </tr>
                    <tr>
                        <td style="color: red">Video pitches for posters</td>
                        <td style="color: red">November 28, 2023 (11:59PM AoE)</td>
                    </tr>
                    <tr>
                        <td>Workshop Date</td>
                        <td>December 15, 2023</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <h3>Scope</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">

            <p>We invite submissions on representation and generative learning over tables, related to any of the following topics:
            </p>

            <ul>
                <li><b>Representation Learning over tables  </b> which can be structured as well as semi-structured, and extend to full databases. 
                    Example contributions are new model architectures, data encoding techniques, pre-training,
                    fine-tuning, and prompting strategies, multi-task learning, etc.
                </li>
                <li><b>Generative Learning and LLMs  </b>  for structured data and interfaces to structured data
                    (e.g. queries, analysis).
                </li>
                <li><b>Multimodal learning  </b>
                    where tables are jointly embedded with, for example, natural language, code (e.g. SQL),
                    knowledge bases, visualizations/images.
                </li>
                <li><b>Downstream Applications  </b> of table representations for tasks like data preparation
                    (e.g. data cleaning, validation, integration, cataloging, feature engineering),
                    retrieval (e.g. search, fact-checking/QA, KG construction),
                    analysis (e.g. summarization, visualization, and query recommendation), and (end-to-end) machine learning.
                </li>
                <li><b>Upstream Applications  </b> of table representation models for optimizing table parsers/extraction
                    (from documents, spreadsheets, presentations), storage (e.g. compression, indexing)
                    and query processing e.g. query optimization
                </li>
                <li><b>Production challenges  </b>of table representation models.
                    Work addressing the challenges of maintaining and managing TRL models in fast evolving contexts,
                    e.g. data updating, error correction, and monitoring, and other industry challenges such as privacy, personalization performance, etc.
                </li>
                <li><b>Domain-specific challenges  </b>for learned table models often arise in domains such as enterprise,
                    finance, medical, law. These challenges pertain to table content, table structure, privacy, security
                    limitations, and other factors that necessitate tailored solutions.
                </li>
                <li><b>Benchmarks and analyses  </b>of table representation models, including the utility of language models as base models
                    versus alternatives and robustness regarding large, messy, heterogeneous, or complex tables.
                </li>
                <li><b>Others: </b> Formalization, surveys, datasets, visions, and reflections to structure and guide future
                    research.
                </li>
            </ul>
        </div>
    </div>

    <hr />

    <div class="row" id="guidelines">
        <div class="col-xs-12">
            <h2>Submission Guidelines</h2>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <h3>Submission link</h3>
            Submit your (anonymized) paper through OpenReview at: <b><a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL" target="blank">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL</a></b><br>
            Please be aware that accepted papers are expected to be presented at the workshop in-person.
        </div>

        <div class="col-xs-12">
            <br>
            <h3>Formatting guidelines</h3>
            The workshop accepts regular research papers and industrial papers of the following types:
            <ul>
                <div class="col-xs-12">
                    <li>Short paper: 4 pages + references.</li>
                    <li>Regular paper: 8 pages + references.</li>
                </div>
            </ul>
            <br>
            <br>
            Submissions should be anonymized and follow the NeurIPS style files, but can exclude the checklist.
            Non-anonymous preprints are no problem, and artifacts do not have to be anonymized. Just submitting the paper without author names/affiliations is sufficient.
            Supplementary material, if any, may be added in the appendix.
            The footer of accepted papers should state “Table Representation Learning Workshop at NeurIPS 2023”. We expect authors to adopt an inclusive and diverse writing style.
            The <a href="https://dbdni.github.io/pages/inclusivewriting.html" target="blank">“Diversity and Inclusion in Writing”</a> guide by the DE&I in DB Conferences effort is a good resource.
        </div>
        
        <div class="col-xs-12">
            <br>
            <h3>Review process</h3>
            Papers will receive light reviews in a double-anonymous manner.
            All accepted submissions will be published on the website and made public on OpenReview but the workshop is non-archival (i.e. without proceedings).
        </div>

        <div class="col-xs-12">
            <br>
            <h3>Novelty and conflicts</h3>
            The workshop does not accept submissions that have previously been published at NeurIPS or other machine learning venues.
            However, we do welcome submissions that have been published in, for example, data management or natural language processing venues.
            We rely on OpenReview for handling conflicts, so please ensure that the conflicts in every author's OpenReview profile are complete, in particular, with respect to the organization and program committees.
            <br>
        </div>

        <div class="col-xs-12">
            <br>
            <h3 style="color: red">Camera-ready instructions</h3>
            Camera-ready papers are expected to express the authors and affiliations on the first page, and state "Table Representation Learning Workshop at NeurIPS 2023'' in the footer.
            The camera-ready version may exceed the page limit for acknowledgements or small content changes, but revision is not required (for short papers: please be aware of novelty requirements of archival venues, e.g. SIGMOD, CVPR).
            The camera-ready version should be submitted through OpenReview (submission -> edit -> revision), and will be published on OpenReview and this website. Please make sure that all meta-data is correct as well, as it will be imported to the NeurIPS website.
        </div>

        <div class="col-xs-12">
            <br>
            <h3 style="color: red">Presentation instructions</h3>
            All accepted papers will be presented as poster during one of the poster sessions (TBA). For poster printing, please refer to the poster instructions on <a href="https://neurips.cc/FAQ/PosterInformation" target="blank">the NeurIPS site</a>.
            Optional: authors of all submissions are also invited to send a teaser video of max. 3 minutes (.mp4) to m.hulsebos@uva.nl, which will be hosted on the website and <a href="https://www.youtube.com/channel/UCON8k4OzdrX9iswYIwb14jQ" target="blank"> YouTube channel</a> of the workshop.
            <br>
            Papers selected for spotlight talks are also asked to prepare a talk of 6 minutes (+1 min Q&A), and upload their slides through the "slides" field in OpenReview. The timeslots for the spotlights will be published soon.
        </div>
    </div>

    <hr />

    <!-- Organizers -->
    <div class="row" id="organization">
        <div class="col-xs-12">
            <h2>Organization</h2>
        </div>

    <div class="col-xs-12">
        <h3>Workshop Chairs</h3>
        <br>
        <div class="col-xs-6 col-lg-3">
            <a href="https://www.madelonhulsebos.com/" target="blank">
                <img class="people-pic" src="assets/people/mh.jpg">
            </a>
            <div class="people-name">
                <a href="https://www.madelonhulsebos.com/" target="blank">Madelon Hulsebos</a>
                <h6>University of Amsterdam</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://www.microsoft.com/en-us/research/people/hadong" target="blank">
                <img class="people-pic" src="assets/people/hd.jpg">
            </a>
            <div class="people-name">
                <a href="https://www.microsoft.com/en-us/research/people/hadong" target="blank">Haoyu Dong</a>
                <h6>Microsoft Research Asia</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://bojan.ninja" target="blank">
                <img class="people-pic" src="assets/people/bk.jpg">
            </a>
            <div class="people-name">
                <a href="https://bojan.ninja" target="blank">Bojan Karlas</a>
                <h6>Harvard</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://cs.stanford.edu/people/lorr1" target="blank">
                <img class="people-pic" src="assets/people/lo.jpg">
            </a>
            <div class="people-name">
                <a href="https://cs.stanford.edu/people/lorr1" target="blank">Laurel Orr</a>
                <h6>Numbers Station AI</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://research.google/people/PengchengYin/" target="blank">
                <img class="people-pic" src="assets/people/py.jpg">
            </a>
            <div class="people-name">
                <a href="https://research.google/people/PengchengYin/" target="blank">Pengcheng Yin</a>
                <h6>Google DeepMind</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://gael-varoquaux.info/" target="blank">
                <img class="people-pic" src="assets/people/gv.jpg">
            </a>
            <div class="people-name">
                <a href="https://gael-varoquaux.info/" target="blank">Gaël Varoquaux</a>
                <h6>INRIA</h6>
            </div>
        </div>
        <div class="col-xs-6 col-lg-3">
            <a href="https://siviltaram.github.io/" target="blank">
                <img class="people-pic" src="assets/people/ql.png">
            </a>
            <div class="people-name">
                <a href="https://siviltaram.github.io/" target="blank">Qian Liu</a>
                <h6>Sea AI Lab</h6>
                <br>
            </div>
        </div>
    </div>

    <div class="col-xs-12">
        <h3>Program Committee</h3>
        Vadim Borisov,	University of Tuebingen<br>
        Paul Groth,	University of Amsterdam<br>
        Wensheng Dou,	Institute of Software Chinese Academy of Sciences<br>
        Hiroshi Iida,	The University of Tokyo<br>
        Sharad Chitlangia,	Amazon<br>
        Jaehyun Nam,	KAIST<br>
        Jinyang Li,	The University of Hong Kong<br>
        Gerardo Vitagliano,	Hasso Plattner Institute<br>
        Rajat Agarwal,	Amazon<br>
        Micah Goldblum,	New York University<br>
        Yury Gorishniy,	Yandex Research<br>
        Roman Levin,	Amazon<br>
        Bhavesh Neekhra,	Ashoka University<br>
        Sebastian Schelter,	University of Amsterdam<br>
        Qingping Yang,	University of Chinese Academy of Sciences<br>
        Matteo Interlandi,	Microsoft<br>
        Tianji Cong,	University of Michigan<br>
        Xiang Deng,	Google<br>
        Beliz Gunel,	Google<br>
        Qian Liu,	Sea AI Lab<br>
        Shuaichen Chang,	Ohio State University<br>
        Zhoujun Cheng,	Shanghai Jiaotong University<br>
        Roee Shraga,	Worcester Polytechnic Institute<br>
        Yi Zhang,	AWS AI Labs<br>
        Xi Rao,	ETH Zurich<br>
        Liane Vogel, Technical University of Darmstadt<br>
        Aneta Koleva, University of Munich / Siemens<br>
        Ivan Rubachev, HSE University / Yandex<br>
        Meghana Moorthy Bhat, Salesforce Research<br>
        José Cambronero, Microsoft<br>
        Till Döhmen, MotherDuck / University of Amsterdam<br>
        Noah Hollman, Charité Berlin / University of Freiburg<br>
        Julian Martin Eisenschlos, Google<br>
        Paolo Papotti, Eurecom<br>
        Zhiruo Wang, Carnegie Mellon University<br>
        Mukul Singh, Microsoft<br>
        Zezhou Huang, Columbia University<br>
        Carsten Binnig, TU Darmstadt<br>
        Linyong Nan, Yale<br>
        Shuo Zhang, Bloomberg<br>
        Alejandro Sierra Múnera, Hasso Plattner Institute<br>
        Qian Liu, Sea AI Labs<br>
        Anirudh Khatry, Microsoft<br>
        Haoyu Dong, Microsoft<br><br>
    </div>


    <!-- <div class="col-xs-12">
        <h3>Review for TRL?</h3>
        We are compiling a PC for light reviewing of submissions to TRL 2023 (at most 3 reviews per reviewer).<br>
        We invite researchers with expertise relevant to TRL to express their interest in reviewing through the below form.<br>
        Your time and expertise is much appreciated!
        <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSdjlbY-590PzDNBtOlUlCzxD_Uhbkmq389pRieW7L5B7F31_g/viewform?embedded=true" width="680" height="320" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
        <br>
    </div> -->

    </div>

    <hr />

    <div class="row" id="accepted-papers">
        <div class="col-xs-12">
            <h2>Accepted Papers</h2>
            <!-- <p>Note: 2 additional papers were accepted but are not listed here because of an anonymity period.</p> -->
        </div>
    </div>

    <br/>

    <div>
        <h3>2023</h3>

        <br/>

        <b style="color: red">An early list of accepted papers is <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/TRL">here</a></b>
    </div>

    <br/>
    <br/>

    <div>
        <h3>2022</h3>

        <br/>

        <h4>Oral</h4>

        <ul class="paper-list">
        <li>
            <a class="paper-title" href="../../assets/papers/analysis_of_the_attention_in_t.pdf" target="_blank">Analysis of the Attention in
                Tabular
                Language Models</a> &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993349" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
            <br>
            <span class="paper-authors">Aneta Koleva, Martin Ringsquandl, Volker Tresp</span>
            <br>
        </li>
        </li>

        <li> <a class="paper-title" href="assets/papers/transfer_learning_with_deep_ta.pdf" target="_blank">Transfer Learning with Deep
                Tabular
                Models</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993348" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
            <br>
            <span class="paper-authors">Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C. Bayan
                Bruss,
                Tom
                Goldstein, Andrew Gordon Wilson, Micah Goldblum</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stable_table_generation_framew.pdf" target="_blank">STable: Table Generation
                Framework
                for
                Encoder-Decoder Models</a>
                    &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993352" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>

                    <br>
            <span class="paper-authors">Michał Pietruszka, Michał Turski, Łukasz Borchmann, Tomasz Dwojak, Gabriela
                Pałka,
                Karolina Szyndler, Dawid Jurkiewicz, Łukasz Garncarek</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/tabpfn_a_transformer_that_solv.pdf" target="_blank">TabPFN: A Transformer That
                Solves
                Small
                Tabular Classification Problems in a Second</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993350" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">Noah Hollmann, Samuel Müller, Katharina Eggensperger, Frank Hutter</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/towards_parameter_efficient_au.pdf" target="_blank">Towards Parameter-Efficient
                Automation
                of Data Wrangling Tasks with Prefix-Tuning</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38993351" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">David Vos, Till Döhmen, Sebastian Schelter</span>
        </li>

        <li> <a class="paper-title" href="https://openreview.net/forum?id=7q_-aEdnGZw" target="_blank">RegCLR: A Self-Supervised Framework
                for
                Tabular Representation Learning in the Wild</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://slideslive.com/38996604" target="blank"><i class="fa fa-play" aria-hidden="true"></i> recording</a>
                <br>
            <span class="paper-authors">Weiyao Wang, Byung-Hak Kim, Varun Ganapathi</span>
        </li>
        </ul>

        <br/>

        <h4>Poster</h4>
        
        <br/>

        <ul class="paper-list">
        <li> <a class="paper-title" href="assets/papers/saint_improved_neural_networks.pdf" target="_blank">SAINT: Improved Neural Networks
                for
                Tabular Data via Row Attention and Contrastive Pre-Training</a><br>
            <span class="paper-authors">Gowthami Somepalli, Avi Schwarzschild, Micah Goldblum, C. Bayan Bruss, Tom
                Goldstein</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/generic_entity_resolution_mode.pdf" target="_blank">Generic Entity Resolution
                Models</a><br>
            <span class="paper-authors">Jiawei Tang, Yifei Zuo, Lei Cao, Samuel Madden</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/towards_foundation_models_for_.pdf" target="_blank">Towards Foundation Models for
                Relational
                Databases</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=GyeGQGmTv30" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                <br>
            <span class="paper-authors">Liane Vogel, Benjamin Hilprecht, Carsten Binnig</span>
        </li>


        <li> <a class="paper-title" href="assets/papers/diffusion_models_for_missing_v.pdf" target="_blank">Diffusion models for missing
                value
                imputation in tabular data</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=URlh7KJfXzM" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                <br>
            <span class="paper-authors">Shuhan Zheng, Nontawat Charoenphakdee</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stab_self_supervised_learning_.pdf" target="_blank">STab: Self-supervised Learning
                for
                Tabular Data</a><br>
            <span class="paper-authors">Ehsan Hajiramezanali, Max W Shen, Gabriele Scalia, Nathaniel Lee Diamant</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/caspr_customer_activity_sequen.pdf" target="_blank">CASPR: Customer Activity
                Sequence
                based
                Prediction and Representation</a><br>
            <span class="paper-authors">Damian Konrad Kowalczyk, Pin-Jung Chen, Sahil Bhatnagar</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/conditional_contrastive_networ.pdf" target="_blank">Conditional Contrastive
                Networks</a><br>
            <span class="paper-authors">Emily Mu, John Guttag</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/self_supervised_representation.pdf" target="_blank">Self-supervised Representation
                Learning
                Across Sequential and Tabular Features Using Transformers</a><br>
            <span class="paper-authors">Rajat Agarwal, Anand Muralidhar, Agniva Som, Hemant Kowshik</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/the_need_for_tabular_represent.pdf" target="_blank">The Need for Tabular
                Representation
                Learning: An Industry Perspective</a><br>
            <span class="paper-authors">Joyce Cahoon, Alexandra Savelieva, Andreas C Mueller, Avrilia Floratou, Carlo
                Curino,
                Hiren Patel, Jordan Henkel, Markus Weimer, Roman Batoukov, Shaleen Deep, Venkatesh Emani, Richard
                Wydrowski,
                Nellie Gustafsson</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/stunt_few_shot_tabular_learnin.pdf" target="_blank">STUNT: Few-shot Tabular Learning
                with
                <span class="paper-authors">Self-generated Tasks from Unlabeled Tables</a><br>
            Jaehyun Nam, Jihoon Tack, Kyungmin Lee, Hankook Lee, Jinwoo Shin</span></li>

        <li> <a class="paper-title" href="assets/papers/tabular_data_generation_can_we.pdf" target="_blank">Tabular Data Generation: Can We
                Fool
                XGBoost?</a>
            <br>
            <span class="paper-authors">EL Hacen Zein, Tanguy Urvoy</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/sima_federating_data_silos_usi.pdf" target="_blank">SiMa: Federating Data Silos using
                GNNs</a>&nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=y4ZOobI1v2w" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                    <br>
            <span class="paper-authors">Christos Koutras, Rihan Hai, Kyriakos Psarakis, Marios Fragkoulis, Asterios
                Katsifodimos

        <li> <a class="paper-title" href="assets/papers/self_supervised_pre_training_f.pdf" target="_blank">Self Supervised Pre-training for
                Large Scale Tabular Data</a><br>
            <span class="paper-authors">Sharad Chitlangia, Anand Muralidhar, Rajat Agarwal</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/rotar_efficient_row_based_tabl.pdf" target="_blank">RoTaR: Efficient Row-Based Table
                Representation Learning via Teacher-Student Training</a><br>
            <span class="paper-authors">Zui Chen, Lei Cao, Samuel Madden</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/mapqa_a_dataset_for_question_a.pdf" target="_blank">MapQA: A Dataset for Question
                Answering on Choropleth Maps</a><br>
            <span class="paper-authors">Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, Ningchuan
                Xiao</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/met_masked_encoding_for_tabula.pdf" target="_blank">MET: Masked Encoding for Tabular
                Data</a><br>
            <span class="paper-authors">Kushal Alpesh Majmundar, Sachin Goyal, Praneeth Netrapalli, Prateek Jain</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/active_learning_with_tabular_l.pdf" target="_blank">Active Learning with Table
                Language
                Models</a><br>
            <span class="paper-authors">Martin Ringsquandl, Aneta Koleva</span>
        </li>

        <li> <a class="paper-title" href="assets/papers/structural_embedding_of_data_f.pdf" target="_blank">Structural Embedding of Data
                Files
                with MAGRITTE</a>
                &nbsp; &nbsp; <a style="border-bottom: 0;" href="https://www.youtube.com/watch?v=_seBQIBzFoI" target="blank"><i class="fa fa-play" aria-hidden="true"></i> video
                    pitch</a>
                    <br>
            <span class="paper-authors">Gerardo Vitagliano, Mazhar Hameed, Felix Naumann</span>
        </li>
        </ul>

    </div>